
<html>
  <head><title>Gemini</title>
 <link rel="stylesheet" type="text/css" href="css/rule-game.css"/>
  </head>  
  <body>
    <h1>Making Gemini play the GOHR</h1>

    <div align="center"><em>Updated 2025-07-14 for Game Server ver 8.020 and <tt>gemini-2.5-flash-preview-05-20</tt></em></div>

    <h2>Architecture</h2>

    <p>The setup that we have for getting Google Gemini to play the Rule Game (aka the GOHR) is, in a way, the inverse of that of the <a href="captive.html">Captive Game Server</a> (CGS). With the CGS, the control resides in the ML (AI) program that wants to solve a GOHR problem; whenever the ML program makes a move, it sends its proposed move as a request to the CGS, and then the CGS applies the move to the current board and returns its decision (accept/deny) in the response. With our Gemini app, the control resides in our app; the app keeps sending requests to the Gemini server at Google, asking it to make its next move. The Gemini's proposed move, hopefully, arrives in the response, and our app then applies it to the board, and prepares to send to Gemini the next request for a move.

      <h2>Gemini models</h2>

    <p>As of the early 2025, Gemini offers a number of "models". We started in March 2025 with the then current model named <tt>gemini-2.0-flash</tt>, which produced very disappointing results.

    <p>Later we switched to a more recent model, <tt>gemini-2.5-flash-preview-05-20</tt>, which Google bills as a <a href="https://ai.google.dev/gemini-api/docs/thinking">model with thinking support</a>. It produces much better results, also works much more slowly.

    <p>
      Out harness tool can be configured to use other models as well (with the command-line option <tt>model=...</tt>).

      <h2>Conversation structure</h2>
      
    <p>A Gemini user could easily imaginge that  when a person or an app interacts with the Gemini server, the server somehow keeps track of the entire content f the "conversation",  perhaps by means of creating some kind of a "model"      of the knowledge conveyed to it by the user's input, in a suitable      internal representation,      and expanding and updating that model as more user input comes through.


    <p>Somewhat unexpectedly, Gemini documentation
      (see e.g. <a href="https://ai.google.dev/api/generate-content#v1beta.GenerateContentResponse">Home |
Gemini API  |
Models | 
API Reference |
Generating content</a>)      implies that nothing like this takes place! The Gemini server is essentially stateless, meaning that when a person or an app interacts with the Gemini server it, the server does not keep track of the user's previous requests, and does not store any "model" of that knowledge.

<p>Instead, if you desire to have a "conversation" with the Gemini bot, you need to supply the entire record of the conversation (what you said -- what the bot said -- what you said -- ... -- what you say now) in each of your HTTP requests! The JSON structure that one sends to the server in the request provides support for "packing" one's conversation into it.

      <p>The GOHR, of course, is built on a dialogue, with the player and the Game Server typically exchanging a request/response pair (in whichever direction) per move. It is possible to literally pack one's dialogue into the Gemini request structure, using the framework provided. But since we are already quite inefficient, the decision was made to normally pack the entire transcript of a series (all episodes played in a single run of our app, on the same rule set) into a single prompt. Certain limited exceptions exist though, and are discussed in appropriate places below.

	<h3>Asking Gemini about this</h3>
	<p>If asked, Gemini will tell all of this to you itself:
	<a href="https://g.co/gemini/share/a08eb7c33222">session transcript</a>

	<p>
	If you want to spare yourself reading our long and verbose conversation, here are a few salient points:

	<p>
1) If you communicate with Gemini using the plain HTTP API, then yes, you have to explicitly include the same system instructions in each API call. If you use the Python API or Node.js API, then, on the surface level, it gives you the concept of a session, so you put your system instructions into the higher-level API only once. But what the higher-level API does behind the scenes is simply "prepending this instruction to the conversation history for you in subsequent sendMessage() calls." So in reality the system instructions are still sent over the wire on every API call, exactly the same way as with the plain HTTP API.

<p>
2) If I use the cached content API, then of course I send the introductory text (the meta-rules, or content of previous episodes) over the wire only once. But internally,  "in terms of the [Gemini] model's core understanding and generation, the behavior should be largely identical" to what happens if no caching has been used.

<p>
3) I asked how the cached content is actually handled: is it stored simply as plain text to be parsed again and again, as some kind of parsed-text structure, or in a more sophisticated way? The answer was, quite a bit of pre-processing is done, so that the cached text's tokens "are then converted into numerical vector embeddings. These embeddings are high-dimensional mathematical representations that capture the semantic meaning of the tokens and their relationships within the context. This is the crucial step where the "text" is transformed into something the neural network can directly work with." Additionally, something called the Key-Value (KV) cache for Gemini's "attention mechanism" is filled.

<p>
So this is quite a bit of data processing work, and caching is helpful with reducing the overall  amount  of work in a long session. But again, it appears what happens is that caching simply eliminates the duplication of some work; the results of the work would be roughly the same with and without caching.

    <h2>Using the app</h2>


    <p>The main script is in <tt>/home/vmenkov/w2020/game/scripts/gemini.sh</tt>. The usage is very similar to that of <a href="captive.html"><tt>captive-full.sh</tt></a>; all the same arguments and options describing the rule set and the board generation are accepted.

      <h3>Options</h3>

      <p>
      There are a number of additional options you can put on the command line, in the <em>key=value</em> format, as follows:

      <ul>

	<li>keyFile=/opt/w2020/gemini-api-key.txt  -- the location of the file with the Gemini API key

	<li>model=gemini-2.0-flash -- the name of the Gemini model to use

	<li>instructionsFile=/opt/w2020/game-data/gemini/system.txt -- the name of the text file with the "system instructions", which are set to the Gemini server with each request
	  
	<li>wait=4000 -- for how long (measured in milliseconds) the client waits between making two requests to the Gemini server. Since most models allow at most 15 queries per minute for free-tier users. The value of 4000 (i.e. 4 sec) is appropriate.

	  <li>max_boards=10 -- the max number of episodes to run with each rule set
	  <li>max_requests=0 -- the max number of requests the client is allowed to make. (If no retries are ever needed, this is equal to the max total number of moves in all episodes). The default value, 0, means "no limit". If you feel the client sometimes likes to loop making the same request over and over, set this to something like 1000.
      </ul>
      
     <h2>Observations - Gemini 2.0</h2>

      <p><strong>This section was written when working with Gemini 2.0 Flash; unless otherwise indicated, everything here is only applied to Gemini 2.0. The problems described in it don't seem to appear to the same extent in Gemini 2.5 Flash; so this section is mostly irrelevant now.</strong>
	  
     <strike>
   <p>When dealing with a system whose main strength is in generating natural language, one has to be ready to handle sensibly a great variety of situations. We don't intend, after all, to teach our own app to understand under natural language (such as Gemini produces); instead we put the bulk of our effort into getting Gemini to produce output in a formalized structured way that would be easy for our app to understand.
     
   <p>As mentioned above, the instructions I provide to Gemini emphasize that what I want to hear from it are descriptions of proposed moves in a standard format, <tt>MOVE objectId bucketId</tt>. On simpler problems, such as color matching, this is more or less what happens; although every once in a while Gemini decides to provide a long disquisition on its motivation and reasoning, its long message still includes the "MOVE ..." substring which our app extracts and uses as the next move. However, in more complicated problems (e.g. "counterclockwise") one can sometimes see Gemini slide into purely verbal descriptions of the move.

     <P>
     .... use 1 retry with an extra prompt ....

     <p>
       .... move already removed piece ...


... artifact of my reading -- he just talks about reasoning first, mentioning old moves, and then gives the new move at the end! ...
       
<p>... "IMMOVABLE" means that the object you have tried to move cannot at present be moved to any bucket, and the object stays on the board. This mean that you should not try to attempt to move this object again until the board has changed in some way (i.e. some other object has been successfully removed).

  <h3>Failing to realize that a piece has been removed already</h3>

<p>This is rare, but has been seen on occasion in Gemini model 2.0. This is reported as code -10 (invlaid object ID) on our side.

<p>
  Seemingly unprovoked. E.g., in gemini-sm_qcts-03.txt:
  <pre>
    During episode 5, you have made so far the following 15 move attempts, with the following results:
MOVE 8 3 ACCEPT
MOVE 6 3 DENY
MOVE 6 0 ACCEPT
MOVE 5 2 DENY
MOVE 4 0 DENY
MOVE 4 0 DENY
MOVE 4 2 ACCEPT
<strong>MOVE 5 0 ACCEPT</strong>
MOVE 0 2 ACCEPT
MOVE 1 1 ACCEPT
MOVE 3 1 DENY
MOVE 3 1 DENY
MOVE 3 1 DENY
MOVE 3 0 ACCEPT
MOVE 2 2 ACCEPT
YOUR MOVE?
Response text={Based on the previous episodes, I hypothesize the following:

*   **Bucket 0:** Accepts Squares
*   **Bucket 1:** Accepts Circles
*   **Bucket 2:** Accepts Triangles
*   **Bucket 3:** Accepts Stars

<strong>Therefore, I will now try to move the remaining object, which is a Blue Square with ID 5, to Bucket 0.</strong>

MOVE 5 0
}
    </pre>

  So the bot for some reason repeats its earlier successful move, <tt>MOVE 5 0</tt>, even though it's staring right at him in the episode's transcript. I will try to help this problem in ver 8.006, by explicitly showing the current state of the board to the bot after each move.
</strike>
  
  <h3>Repeating a wrong move</h3>

<p>With the model <tt>gemini-2.0-flash</tt>, this happens both on code 7 (IMMOVABLE) and code 4 (DENY). With Gemini 2.5 Flash, this is much less common; unless otherwise indicated, all examples below are from 2.0.
  
  <pre>
%    grep At gemini-ordL1-01.txt
    ...
At 2025-04-01 11:23:46, Moving piece 0 to bucket 0. Code=7
At 2025-04-01 11:23:51, Moving piece 0 to bucket 0. Code=7
At 2025-04-01 11:23:57, Moving piece 0 to bucket 0. Code=7
At 2025-04-01 11:24:02, Moving piece 0 to bucket 0. Code=7
At 2025-04-01 11:24:08, Moving piece 0 to bucket 0. Code=7
    ...
  </pre>

  (ccw)
  <pre>
At 2025-04-01 11:48:43, Moving piece 6 to bucket 0. Code=4
At 2025-04-01 11:48:49, Moving piece 6 to bucket 0. Code=4
</pre>

<p>Sometimes this repetition happens just 2-3 times; but sometimes you can see the bot repeating the same failed move 30 times, but then, fortunately, trying something else.

<p>In another twist on the same theme, sometimes Gemini alternates between seveveral failed attempts, cycling between them endlessly:
  <pre>
    ...
At 2025-06-12 01:14:07, Moving piece 3 to bucket 3. Code=4
At 2025-06-12 01:14:14, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:14:21, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:14:28, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:14:35, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:14:42, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:14:49, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:14:55, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:02, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:09, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:16, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:24, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:31, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:38, Moving piece 1 to bucket 1. Code=4
At 2025-06-12 01:15:45, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:52, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:15:59, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:16:05, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:16:12, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:16:19, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:16:26, Moving piece 1 to bucket 1. Code=4
At 2025-06-12 01:16:33, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:16:40, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:16:47, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:16:55, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:17:02, Moving piece 1 to bucket 3. Code=4
At 2025-06-12 01:17:09, Moving piece 1 to bucket 1. Code=4
At 2025-06-12 01:17:16, Moving piece 1 to bucket 1. Code=4
At 2025-06-12 01:17:23, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:17:30, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:17:37, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:17:43, Moving piece 1 to bucket 0. Code=4
At 2025-06-12 01:17:50, Moving piece 1 to bucket 1. Code=4
At 2025-06-12 01:17:57, Moving piece 1 to bucket 0. Code=4
...
</pre>
  
<p>
This despite me  explicitly telling the bot (in the "system instructions") not to repeat a failed move until the board changes.

<p><Strong>With Gemini 2.5 Flash</strong>, this was so far only seen in a particularly difficult problem (which Gemini 2.5 wasn't able to solve in 300+ moves).
  <pre>
%    grep Movin gemini-colOrdL1_BRKY-250614-1520.txt
At 2025-06-14 23:41:48, Moving piece 8 to bucket 1. Code=0<strong>
At 2025-06-14 23:43:35, Moving piece 0 to bucket 2. Code=7
At 2025-06-14 23:45:55, Moving piece 1 to bucket 3. Code=7
At 2025-06-14 23:48:10, Moving piece 3 to bucket 1. Code=7
At 2025-06-14 23:49:41, Moving piece 0 to bucket 2. Code=7
At 2025-06-14 23:50:54, Moving piece 0 to bucket 2. Code=7
At 2025-06-14 23:52:29, Moving piece 3 to bucket 1. Code=7
At 2025-06-14 23:55:23, Moving piece 0 to bucket 2. Code=7
</strong>
At 2025-06-14 23:56:49, Moving piece 4 to bucket 1. Code=0
   </pre>
In the above example, the sequence of 7 invalid moves (code 7 means IMMOVABLE) includes only 3 unique movements
  
<p>Will try reminding about it ...

     <h2>Timing and quotas</h2>


<p>Since at the moment (April-June 2025) we are using Gemini as free-tier subscribers, we have to abide by the server's fairly strict <a href="https://ai.google.dev/gemini-api/docs/rate-limits">usage quotas</a>.
       
<h3>Gemini 2.0 Flash</h3>
     
<p>Gemini 2.0 was pretty fast, so the main limiting factor was the free-tier quota, 15 moves per minute.
By  sleeping 5 seconds between requeststs, we by and large avoid hitting that limit.   The daily quota for the free-tier users is 1500 requsts per day, which can be easily reached if you don't use let the bot to stupidly repeat the same bad move forever. This is why I usually limited Gemini 2.0 runs to, say, 600 requests.


  <p>
As almost everything with the Gemini server, the limits and the way to reacting to the breaching of them are fluid, meaning that you cannot expect the same reaction every time. When the Gemini server decides that the limit is reached, it may respond with a detailed message, indicating how many seconds we should wait before retrying, in order to return to its good graces. Or it may send a more general message, in which case we try to sleep for 1 minute.

  <h3>Gemini 2.5 Flash</h3>
	<p>In contrast, requests with Gemini 2.5 took a much longer time (typically, in the 30-120 sec, with the time per request usually slightly increasing as the game progresses), at least for a free-tier user. Thus the quota limits (10 request per minute, and 250,000 tokens per minute) are pretty much irrelevant. The daily limit, 500 requests per day, still can be achieved after a few hours of working.

		    
     
  <h2><a name="FDCL">Gemini results on the rule sets from the FDCL/basic suite</a></h2>

<p>The results for the newer model, <tt>gemini-2.5-flash-preview-05-20</tt> are usually much better than for <tt>gemini-2.0-flash</tt>, which rarely accomplishes anything of note. For each rule set below, unless two models are explicitly compared, the results are for the newer, better model.

<p>In general, it seems that Gemini 2.5 Flash achieves Paul's suggested "mastery criterion" (10 successful move attempts in a row) on most of the rule sets from the FDCL/basic suite that we have tried. It does it fairly quickly, typically in under 100 moves (half a dozen episodes or so), and sometimes much faster than that. (Of course, since a Gemini 2.5 response takes up to 1-2 minutes to obtain, at least for free tier subscribers, we are still talking about the clock time on the order of an hour or several hours per problem).

<p>When we end the run after the "mastery criterion" has been achieved, we explcitly ask the Gemini bot what it thinks the hidden rule set is, and it gives us a detailed explanation. Gemini 2.5 Flash being quite "talkative", it also fairly often volunteers an exlpanation of its current idea of the hidden rule, its confidence in this, and the hypothesi/es it plans to check.

<p><strong>The quality of the picture of the rule set that the Gemini bot has its mind at the point of reaching 10 good moves is highly variable</strong>.
<ul>
  <li>In some simpler cases, it's pretty much what the designers of the FDCL/basic suite had in mind.
  <li>In some others, it's basically the intended rule with some extraneous stuff of no real importance added on it. (E.g. the "discovered" rule has the correct description of what game pieces can be picked, but more restrictive rules on what bucket they can be put into).
  <li>In yet others, the "discovered" rule set would probably explain all (or almost all) moves in the already-played episodes, but absolutely won't be correct for all possible boards. (E.g. the realy rule is color-based, but it's just so happened that a position-based rule would be correct for all presented boards). I think it's quite likely that having the bot play longer (say until 20 good moves in a row, or until some Bayesian mastery criterion is reached) will result in a more correct guess of the rule set.
</ul>

<p>
A few cases where I couldn't get Gemini to reach the 10-good-moves mastery criteria after 200+ moves included 
shaOrdL1_csqt and colOrdL1_BRKY: the two rule sets with the <tt>postpos:</tt> predicate.
This is the kind of rules that weren't available in the original Rule Game; the support for them 
(the <tt>postpos:</tt> predicate) was introduced in early 2025, when Paul proposed to include this kind of rules into the FDCL/basic suite. These are fairly difficult for humans too, since they combine two concepts (color and position) in a somewhat complicated way: the player should not just go for a red piece (or a square) in the topmost row, but should first think of all red pieces (or all squares), then isolate the top row of that set, and then pick the leftmost red piece (or the leftmost square). 


  <h3>quadNearby</h3>

  <p>
The older model, <tt>gemini-2.0-flash</tt> bogs down here (not much progress after 600 moves; lots of repetitions of failed moves, and occasionally even grabbing at the already-vacated cell). OTOH, the new model, <tt>gemini-2.5-flash-preview-05-20</tt> is somehow able to solve this <em>without making a single error</em>.
    
<blockquote>
Based on the successful moves in Episode 1 and the first move in Episode 2, the secret rule appears to be determined solely by the object's position (X, Y) on the board, dividing the board into four 3x3 quadrants, each mapping to a specific bucket. The color and shape of the objects do not seem to play a role.

Here's the rule:

The 6x6 board is divided into four 3x3 quadrants based on the object's (X, Y) coordinates:

  <pre>
1.  **Top-Left Quadrant (X between 1-3, Y between 4-6):** Objects in this quadrant must be moved to **Bucket 0** (X=0, Y=7).
    *   *Example from Episode 1:* Object 6 (x=3, y=5) was moved to Bucket 0.
    *   *Objects currently on board in Episode 2 fitting this:*
        *   Object 3 (BLACK, TRIANGLE, x=3, y=4) -> Bucket 0
        *   Object 5 (BLACK, SQUARE, x=1, y=5) -> Bucket 0
        *   Object 6 (BLACK, SQUARE, x=2, y=5) -> Bucket 0

2.  **Top-Right Quadrant (X between 4-6, Y between 4-6):** Objects in this quadrant must be moved to **Bucket 1** (X=7, Y=7).
    *   *Examples from Episode 1:* Objects 4 (x=4, y=4), 5 (x=5, y=4), 7 (x=4, y=5), 8 (x=5, y=6) were moved to Bucket 1.
    *   *Objects currently on board in Episode 2 fitting this:*
        *   Object 4 (BLACK, CIRCLE, x=5, y=4) -> Bucket 1
        *   Object 7 (BLACK, SQUARE, x=4, y=5) -> Bucket 1
        *   Object 8 (BLUE, CIRCLE, x=5, y=5) -> Bucket 1

3.  **Bottom-Right Quadrant (X between 4-6, Y between 1-3):** Objects in this quadrant must be moved to **Bucket 2** (X=7, Y=0).
    *   *Examples from Episode 1:* Objects 0 (x=4, y=2), 2 (x=4, y=3), 3 (x=5, y=3) were moved to Bucket 2.
    *   *Objects currently on board in Episode 2 fitting this:*
        *   Object 2 (BLUE, CIRCLE, x=5, y=3) -> Bucket 2

4.  **Bottom-Left Quadrant (X between 1-3, Y between 1-3):** Objects in this quadrant must be moved to **Bucket 3** (X=0, Y=0).
    *   *Example from Episode 1:* Object 1 (x=2, y=3) was moved to Bucket 3.
    *   *Example from Episode 2 (your successful move):* Object 0 (x=1, y=1) was moved to Bucket 3.
    *   *Objects currently on board in Episode 2 fitting this:*
        *   Object 1 (YELLOW, TRIANGLE, x=2, y=3) -> Bucket 3
</pre>
  
In summary, the secret rule is to determine which of the four 3x3 coordinate quadrants an object occupies and then move it to the specific b>ucket associated with that quadrant.
</blockquote>

Or, in another run:
<blockquote>
The secret rule appears to be based on the proximity of the object to the bucket corners, using the Manhattan distance.

Here is my understanding of the rule:

1.  **Calculate Manhattan Distance:** For any object at `(X, Y)` on the board, calculate its Manhattan distance to each of the four bucket corner coordinates:
<pre>
    *   **Bucket 0 (X=0, Y=7):** Distance = `|X - 0| + |Y - 7|`
    *   **Bucket 1 (X=7, Y=7):** Distance = `|X - 7| + |Y - 7|`
    *   **Bucket 2 (X=7, Y=0):** Distance = `|X - 7| + |Y - 0|`
    *   **Bucket 3 (X=0, Y=0):** Distance = `|X - 0| + |Y - 0|`
</pre>

2.  **Determine the Target Bucket:** An object can only be moved to the bucket (or buckets, in case of a tie) that has the *minimum* Manhattan distance from the object's current position.

All moves made so far in Episode 1 and the first move in Episode 2 are consistent with this rule. There is no indication that the rule depends on object properties (color, shape) or the history of previous successful moves.
</blockquote>


<P>The termination ("mastery") criterion in all runs was 10 correct moves in a row.

  <h3>quadMixed1</h3>

<p>While the old model bogs down here, the newer model reached the mastery criterion in 36 moves. The reasoning was explaineds as follows:

  <pre>
 The secret rule determines which bucket an object can be placed into based solely on its position (X,Y coordinates) on the board. The board is conceptually divided into four quadrants using the midpoints of the X and Y ranges (X=3.5 and Y=3.5).

Here's the detailed rule:

1.  **Board Quadrants Definition:**
    *   **Top-Left Quadrant (TL):** Cells where `X <= 3` AND `Y >= 4`
    *   **Top-Right Quadrant (TR):** Cells where `X >= 4` AND `Y >= 4`
    *   **Bottom-Right Quadrant (BR):** Cells where `X >= 4` AND `Y <= 3`
    *   **Bottom-Left Quadrant (BL):** Cells where `X <= 3` AND `Y <= 3`

2.  **Bucket Mapping Rule:**
    *   If an object is in the **Top-Left Quadrant (TL)**, it must be moved to **Bucket 1**. (Bucket 1 is located at X=7, Y=7, corresponding to the Top-Right corner).
    *   If an object is in the **Top-Right Quadrant (TR)**, it must be moved to **Bucket 3**. (Bucket 3 is located at X=0, Y=0, corresponding to the Bottom-Left corner).
    *   If an object is in the **Bottom-Right Quadrant (BR)**, it must be moved to **Bucket 2**. (Bucket 2 is located at X=7, Y=0, corresponding to the Bottom-Right corner).
    *   If an object is in the **Bottom-Left Quadrant (BL)**, it must be moved to **Bucket 0**. (Bucket 0 is located at X=0, Y=7, corresponding to the Top-Left corner).

In summary:
*   Objects in the **Bottom-Right** quadrant go to the **Bottom-Right** bucket (Bucket 2).
*   All other objects are moved to the *diagonally opposite* bucket relative to their quadrant, if we consider bucket positions as corresponding to board corners:
    *   Top-Left quadrant -> Top-Right bucket (Bucket 1)
    *   Top-Right quadrant -> Bottom-Left bucket (Bucket 3)
    *   Bottom-Left quadrant -> Top-Left bucket (Bucket 0)

This rule has successfully explained all ACCEPT and DENY outcomes in Episode 1, Episode 2, and the current moves in Episode 3. There have been no IMMOVABLE responses, indicating that all objects are always movable to their designated bucket.
</pre>
    
  
  <h3>sm_qcts (shape match)</h3>

  This is one of the very few rules where even with Gemini 2.0, mastery was demonstrated after ca. 200 move attempts. The explanation has a model more or less consistent with reality, e.g.
  <blockquote><pre>
Based on these observations, here's my hypothesis:
*   **Bucket 0:** Accepts `SQUARE` or `BLACK`
*   **Bucket 1:** Accepts `CIRCLE` or `RED`
*   **Bucket 2:** Accepts `TRIANGLE` or `BLUE`
*   **Bucket 3:** Accepts `STAR` or `YELLOW`
</pre>
  </blockquote>

  With Gemini 2.5 Flash, the bot solved the game in 23 move attempts. The rule description given by the bot was super-overtrained, but apparently mostly correct.

  <pre>

    </pre>
  

  <h3>ordL1 (reading order)</h3>

  <p>
  Gemini 2.5 solved this in 26 moves.

  <pre>
Here's my understanding of the secret rule based on the observed episodes:

The game has two main rules that dictate moves:
1.  **Which object can be moved (Movability Rule):** Only one specific object is "movable" at any given time. If you attempt to move any other object, it will be "IMMOVABLE".
    *   The movable object is determined by its position (X, Y) on the board.
    *   It must have the **highest Y-coordinate** (i.e., be in the highest row) among all objects currently on the board.
    *   If there are multiple objects in that highest row, it must be the one with the **lowest X-coordinate** (i.e., the leftmost object in that row).
    *   This implies a "top-down, then left-to-right" priority for object removal. Objects in lower Y-coordinates (or to the right in the same Y-coordinate) become movable only after all objects above/to their left are removed.

2.  **To which bucket it must be moved (Bucket Assignment Rule):** Once an object is identified as movable, the correct bucket it must be placed into depends solely on its position (X, Y). If you choose the wrong bucket, the response will be "DENY".

    *   **If the object's Y-coordinate is 4 or greater (Y >= 4):**
        *   It must be moved to **Bucket 0** (the bucket located at X=0, Y=7). This applies to all objects in rows 4, 5, and 6.

    *   **If the object's Y-coordinate is 3 or less (Y <= 3):**
        *   **If the object's X-coordinate is 3 or less (X <= 3):**
            *   It must be moved to **Bucket 3** (the bucket located at X=0, Y=0). This applies to objects in rows 1, 2, or 3, and columns 1, 2, or 3.
        *   **If the object's X-coordinate is greater than 3 (X > 3):**
            *   It must be moved to **Bucket 2** (the bucket located at X=7, Y=0). This applies to objects in rows 1, 2, or 3, and columns 4, 5, or 6.

The object's color, shape, or ID, as well as the previous successful move, do not seem to influence which object can be moved or to which bucket it goes.

**Summary in simplified terms:**
*   Always clear the board from top to bottom, and within each row, from left to right.
*   Objects in the top half (rows 4, 5, 6) go to the top-left bucket (Bucket 0).
*   Objects in the bottom half (rows 1, 2, 3) go to either the bottom-left bucket (Bucket 3) if they are in the left half (columns 1, 2, 3), or the bottom-right bucket (Bucket 2) if they are in the right half (columns 4, 5, 6).

I will now proceed with the next move based on this rule.}
</pre>

  <h3>allOfColOrd_BRKY</h3>

  ( Colors in order. All blue before any red, etc.)

  <P> Solved in 51 moves (to 10 good moves), or 89 moves (to 25). After 10 good moves, the complicated explanation contains some unnecessary extraneous rules, with the correct (and sufficient) core rule:
    <blockquote>
    <code> ...
Objects must be cleared from the board in a strict sequence of colors: **BLUE -> RED -> BLACK -> YELLOW** ...
      </code>
    </blockquote>

<p>Weirdly, after 25 good moves the explanation given is entirely spurious, more along the lines of "order L1", although the bot admit that it's not entirely correct.

  <h3>allOfColOrd_KRBY</h3>
  (Similar)

  
  Demonstrated "mastery" (10 good moves in a row) after   47 moves. However, still has not worked out a good idea of the rules by that point. 

  <blockquote>
    Okay, I've analyzed the past episodes and the current state to try and figure out the secret rule. This has been quite a challenge, as some patterns appear and then contradict themselves across episodes. However, I have a working hypothesis that covers most of the observed behavior. ...
  </blockquote>
  That follows by a very complicated theory (overfitting indeed!) which starts with a position-based approach (4 quarters) combined with other things, among which are colors too:
  <blockquote><ul>
      
      Color-based Priority within Active Quadrant (This part is still being refined):
      <ul>
    <li>   Objects with `BLACK` or `BLUE` colors (let's call them "Primary Colors") generally have priority.
    <li>   Objects with `RED` or `YELLOW` colors (let's call them "Secondary Colors") are sometimes `IMMOVABLE` even when they are in the `current_active_quadrant`. This specific condition is where my understanding is still incomplete, as it seems to vary slightly between episodes.      
      </ul>
  </blockquote>

  <p>The bot is fully cognizant of the limitations of the current approach, so if it were told to play longer -- say up to 20 good moves in a row, or up to some kind of R-value (Bayesian) threshold -- it probably would discover the real rule.
  
<h3>allOfShaOrd_csqt</h3>

<pre>
(shape:circle)
(shape:star)
(shape:square)
(shape:triangle)
</pre>

<p>The 10-good-move criterion is achieved after 115 move attempts, but the explanation is largely incorrect. The bot apparently thinks that the movability criteria are different in different episodes.


<h3>allOfShaOrd_qcts</h3>

Achieved the mastery criterion (10 good moves) after 40 moves, but the explanation is entirely wrong (some weird position-based algo). Should try more moves...


<h3>buckets_2130</h3>

Found the correct rule in 26 moves:

<blockquote><pre>
    ...
       More formally:
    *   If `N % 4 == 0` (i.e., the 1st, 5th, 9th, etc., successful move), the object must go to **Bucket 2**.
    *   If `N % 4 == 1` (i.e., the 2nd, 6th, 10th, etc., successful move), the object must go to **Bucket 1**.
    *   If `N % 4 == 2` (i.e., the 3rd, 7th, 11th, etc., successful move), the object must go to **Bucket 3**.
    *   If `N % 4 == 3` (i.e., the 4th, 8th, 12th, etc., successful move), the object must go to **Bucket 0**.
</pre></blockquote>



<h3>cm_KRBY</h3>

Found the correct rule (color matching) after 31 moves.


<h3>cm_RBKY_cw_0123</h3>

(Need to pick colors in order, and use the designated bucket for each one)

Found essentially the correct rule (with some extraneous additions) in 80 moves.

<pre> ...
**Rule 1: Bucket Destination (Color-based)**
This rule determines which bucket an object can be placed into. It is solely based on the object's **color**:
*   **RED** objects must be placed into **Bucket 0** (X=0, Y=7, Top-Left).
*   **BLUE** objects must be placed into **Bucket 1** (X=7, Y=7, Top-Right).
*   **BLACK** objects must be placed into **Bucket 2** (X=7, Y=0, Bottom-Right).
*   **YELLOW** objects must be placed into **Bucket 3** (X=0, Y=0, Bottom-Left).

  *Explanation:*

  * If you attempt to move an object to a bucket that does not match
  its color, you will receive a "DENY" response.

  * All successful "ACCEPT" moves observed across all episodes
  strictly adhere to this color-to-bucket mapping.

**Rule 2: Object Availability (Movability Sequence)**

  This rule determines *which* object is currently movable on the
  board. All other objects will yield an "IMMOVABLE" response if you
  try to move them.

  1.  **First Move of an Episode**: The very first object that can be
  moved in any episode is always the **RED object with the lowest ID**
  among all RED objects currently on the board.
  
  2.  **Subsequent Moves**: After an object is successfully moved to a
  specific bucket (let's call it `Bucket N`), the next object(s) that
  become movable are determined by a cyclical sequence of buckets:
  
    *   If the previous object was placed in **Bucket 0** (RED), then the next movable object(s) will be **BLUE** objects.
    *   If the previous object was placed in **Bucket 1** (BLUE), then the next movable object(s) will be **BLACK** objects.
    *   If the previous object was placed in **Bucket 2** (BLACK), then the next movable object(s) will be **YELLOW** objects.
    *   If the previous object was placed in **Bucket 3** (YELLOW), then the next movable object(s) will be **RED** objects.

     * **Skipping Colors**: If there are no objects of the target color
  (e.g., no BLUE objects left when Bucket 0 was the last target), the
  rule cycles to the next color in the sequence until an available
  object is found (e.g., after Bucket 0, if no BLUE objects, try
  BLACK, then YELLOW, then RED again).

  3.  **Specific Object Selection (within a color group)**: If there
  are multiple objects of the designated "next movable" color on the
  board, only *one specific object* among them is made movable by the
  game at that moment. The exact criteria for selecting this specific
  object (e.g., based on X/Y coordinates, shape, or a specific ID
  ordering other than lowest ID) are not fully apparent from the
  provided logs, as there were inconsistent patterns (sometimes lowest
  ID, sometimes not). However, the "IMMOVABLE" response indicates that
  an object is not the designated one.
</pre>


<h3>col1Ord_BRKY</h3>

No solution after 330 moves. Tantalizingly, the bot did at least once come up with a good explanation... but did not re-generate it on subsequent moves.

<h3>col1OrdBuck_BRKY0213</h3>
<pre>
1 (color:blue, bucket:0)
1 (color:red, bucket:2)
1 (color:black, bucket:1)
1 (color:yellow, bucket:3)
</pre>

The bot reached the mastery criterion (10 good moves) after 104 moves, but admits that it only knows the rules partially: it knows which color goes to which buckets, but is not sure in which order the pieces need to be picked; wants to continue experimenting.


<h3>colOrdL1_BRKY</h3>

<pre>
1 (color:blue, postpos:L1)
1 (color:red, postpos:L1)
1 (color:black, postpos:L1)
1 (color:yellow, postpos:L1)
</pre>


Failed to solve. I interrupted the job after 291 moves, when the bot
seemed to had long been stuck in some wrong concept of the rules,
mostly based on postion (quadrants).  Also, this was a fairly rare case
of Gemini 2.5 repeating failed moves several times.

<h3>cw_0123</h3>


Solved in 15 moves. Described the rules well.


<h3>ordL1_Nearby</h3>

Fully solved in 15 moves. (25 good moves follow as well).


<h3>ordRevOfL1_Nearby</h3>

Fully solved in 95 move attempts. Good explanation given ("snake scan" :-). (25 good moves soon follow as well).


<h3>ordRevOfL1_Remotest</h3>

<pre>
Order RevOfL1=[6,5,4,3,2,1, 12,11,10,9,8,7, 18,17,16,15,14,13, 24,23,22,21,20,19, 30,29,28,27,26,25, 36,35,34,33,32,31]
(pos:RevOfL1, bucket:Remotest)
</pre>

Achieved the 10-good-moves mastery criterion after 158 moves, but the discovered rule is not correct. The bot figured out early on that a piece can only be put to the most remote bucket, and which piece must be picked first. The rules for subsequent pickings make little sense though.


<h3>ordRevOfL1</h3>


Solved in 30 moves. Discovered the correct pick rule (reverse of L1), with a spurious restrictions on destination buckets (it thinks one must use the nearby bucket). The spurious restriction seems to be based on positive evidence (yes of course you can use that bucket), without any negative evidence:
<blockquote>
*   **Bucket Selection (DENY/ACCEPT):**<br>
    *   Across both Episode 1 and Episode 2, every successful move ("ACCEPT") consistently placed the object into the bucket corresponding to its quadrant as defined above. There were no "DENY" responses in the provided logs, but the consistent "ACCEPT" pattern strongly suggests this quadrant-based mapping for bucket selection.
</blockquote>


<h3>sha1OrdBuck_qcts0213</h3>
<pre>
1 (shape:square, bucket:0)
1 (shape:circle, bucket:2)
1 (shape:triangle, bucket:1)
1 (shape:star, bucket:3)
</pre>

Correctly solved in 172 moves. Good description of the rules.

<h3>sha1Ord_csqt</h3>
<pre>
1 (shape:circle)
1 (shape:star)
1 (shape:square)
1 (shape:triangle)
</pre>

The mastery criterion is met after 87 moves, but the hypothesized rules are entirely wrong. (The bot knows that it needs to start an episode with a circle, but then has a complicated position-based rule, which has nothing to do with the true secret rule.) 


<h3>shaOrdL1_csqt</h3>

<pre>
1 (shape:circle, postpos:L1)
1 (shape:star, postpos:L1)
1 (shape:square, postpos:L1)
1 (shape:triangle, postpos:L1)
</pre>


Lots of repeats of a bad move; likely won't solve it. I interrupted it after 200 move attempts.

<h3></h3>
action w2020/gemini-2.5-flash-logs> more  /opt/w2020/game-data/rules/pk/shapeVcolor.txt
# to see whether naive pplayers are more likely to find shape or color match. 
(shape:square,bucket:0) (shape:circle,bucket:1) (shape:triangle,bucket:2) (shape:star,bucket:3)  (color:red,bucket:0) (color:yellow,bucket:1) (color:green,bucket:2) (color:blue,bucket:3)




<h2><a name="cost">Cost estimates</a></h2>

<p>
If we switch to a <a href="https://ai.google.dev/gemini-api/docs/pricing">paid tier</a>, how much would playing a game cost?

<p>For Gemini 2.5 flash model (thinking ones) Google quotes the price of $0.15 per million input tokens, and $3.50 per million output tokens.

<p> A token, more or less, is a word or a punctuation mark. For a medium-difficulty rule set, where the mastery criterion was achieved at 80 moves, and an essentially correct rule was discovered, the stats for the log file look like this: 
<pre>  
~/w2020/gemini_logs-3/victory> grep -c Moving  gemini-cm_RBKY_cw_0123-250613-0953.txt | more
80
~/w2020/gemini_logs-3/victory> wc  gemini-cm_RBKY_cw_0123-250613-0953.txt 
  8397  62689 554826 gemini-cm_RBKY_cw_0123-250613-0953.txt
</pre>

<p>This log consists mostly of the texts of requests and responses. We can separate responses as follows:
<pre>
  perl -pe 's/\n/ /sg' gemini-cm_RBKY_cw_0123-250613-0953.txt > a.tmp
  perl -pe 's/.*?Response text=\{(.+?)\}/$1\n/g' a.tmp > b.tmp

~/w2020/gemini_logs-3/victory>  wc b.tmp 
    81  28682 175880 b.tmp
</pre>

<p>So the rough estimate is that in this session the responses included  28682 words, and requests, the rest, i.e. ca. 34,000 words. Assuming 1.5 tokens per word, that will give us ca. 42,000 tokens in the responses and 51,000 in the requests, for the total cost estimate of
  <div align="center">
  51*0.001 * 0.15 + 42*0.001 * 3.50 = <strong>$0.15</strong>
  </div>
  for playing that game.

<p>As the large number of tokens indicates, thinking models such as Gemini 2.5 Flash are quite talkative, and in response to many requests they explain their reasoning even without me asking them. The cost probably can be reduced quite a bit by explicitly telling the bot NOT to provide any reasoning unless asked.

<p>Google tells us that <blockquote><a href="https://ai.google.dev/gemini-api/docs/caching?lang=rest">Implicit caching is enabled on Gemini 2.5 models by default. If a request contains content that is a cache hit, we automatically pass the cost savings back to you.</blockquote>

 <p>Google also advertises its "Gemini 2.5 Pro Previw" model, which supposedly "excels at coding and complex reasoning tasks". It's priced significantly higher; with that model, playing the same game as above would cost approximately
  <div align="center">
  51*0.001 * 2.50 + 42*0.001 * 15 = <strong>$0.75</strong>.
  </div>

 <p>The cost of a game depends, of course, on how many moves it involved. It's fair to assume that the number of input tokens grows quadratically, and that of output tokens linearly with the number of moves. So if we had a similar game with 800 moves instead of 80, the cost estimate would be
  <div align="center">
  51*100*0.001 * 0.15 + 42*10*0.001 * 3.50 = <strong>$2.20</strong> 
  </div>
for Gemini 2.5 Flash, and   
  <div align="center">
  51*100*0.001 * 2.50 + 42*10*0.001 * 15 = <strong>$19</strong>.
  </div>
for Gemini 2.5 Pro. 

  <h3>And the thinking costs money too...</h3>

  <p>
  Addendum 2025-06-19: I just realized that with the thinking models the pricing is based not only on the visible output tokens but also on the invisible "thought tokens"; those are priced <a href="https://ai.google.dev/gemini-api/docs/thinking">same as output tokens</a>.

  <p>
    Those thinking tokens run up quite quickly: on some runs with temperature=0 I had the "totalTokenCount" (the sum of output token count and thought token count) exceed 67K even before the targetStreak=10 was reached.  So if we have 100 moves needed to solve a problem, with 50k total per move, that's 5 million total tokens; priced at $2.50/million (in G2.5 Flash), that would cost <strong>$12.50</strong> per  problem; with G2.5 Pro (at $10/million), that would be $50 per problem.

    <!--
 FIRST SERVER RESPONSE:
 {"candidates":    [{"content":{"parts":[{"text":"MOVE 0 3"}],"role":"model"},"finishReason":"STOP","index":0}],
  "usageMetadata":{"promptTokenCount":1139,"candidatesTokenCount":5,"totalTokenCount":4438,"promptTokensDetails":[{"modality":"TEXT","tokenCount":1139}],"thoughtsTokenCount":3294},
   "modelVersion":"models/gemini-2.5-flash-preview-05-20",
   "responseId":"GW1UaKCbDo2OjMcPipaA8Qc"}

-->

<H2><a name="prep">Gemini queries with "prepared episodes"</a></h2>

<P>I carried out a series of runs where Gemini was shown 10 prepared episodes fpr each rule and ask to deduce the rule, and another series with 100 episodes perrule.

<p>The runs were conducted with the default temperature.  In each run, I asked Gemini to provide 3 candidate responses. (Gemini being non-deterministic, it can always come up with multiple answers, if asked). However, why the 3 responses could be very different textually, I usually won't see much difference between their degree of correctness. In those cases when the responses were substantially different, I listed the individual grades, seaprated by commas.
  
  <P>The results are in /home/vmenkov/gemini/prepared-10 and /home/vmenkov/gemini/prepared-100

    
<p>The grading key:    

<ul>
  <li>A: good explanation
  <li>A-: good explanation, with some extraneous restrictions that don't hurt
  <li>B+: mostly correct, only a small part erroneous
  <li>B: partially correct
  <li>C+: incorrect, but at least with some right ideas
  <li>C: incorrect (e.g. proposing a position-based theory for a color- or shape-based rul)
</ul>


<table border="1">
  <TR><td>Rule set<td>Bot plays<td>prep 10 <td>prep 100 <td>Notes
<tr><td>allOfColOrd_BRKY<td><td>C<td>C<td>position-based theory
<tr><td>allOfColOrd_KRBY<td><td>B+<td>C,C+,C<td>guessed KBRY instead of KRBY
<tr><td>allOfShaOrd_csqt<td><td>C+<td>C<td>position-based theory, but knows that can start with a circle
<tr><td>allOfShaOrd_qcts<td><td>C+<td>C+<td>knows to start with a square
<tr><td>buckets_2130<td><td>A<td>C,A,B-
<tr><td>ccw<td><td>A<td>A
<tr><td>cm_KRBY<td><td>A<td>A
<tr><td>cm_RBKY<td><td>A<td>C,C,B
<tr><td>cm_RBKY_cw_0123<td><td>A<td>B+,A,A
<tr><td>col1Ord_BRKY<td><td>C<td>C<td>Talks about target buckets
<tr><td>col1OrdBuck_BRKY0213<td><td>B<td>B-,C,B
<tr><td>col1OrdBuck_BRKY3120<td><td>B+<td>
<tr><td>col1Ord_KRBY<td><td>C <td>
<tr><td>colOrdL1_BRKY<td><td>C<td>
<tr><td>colOrdL1_KBYR<td><td>C<td>
<tr><td>cw_0123<td><td>A<td>
<tr><td>cw<td><td>A<td>
<tr><td>ordL1<td><td>A-<td><td>Describes the correct L1 rule, but in terms of "likelihood", rather than certainty
<tr><td>ordL1_Nearby<td><td>C+,B-<td>
<tr><td>ordRevOfL1<td><td>C<td>
<tr><td>ordRevOfL1_Nearby<td><td>C+<td><td>Figures the bucket assignment, but not the RevOfL1 order
<tr><td>ordRevOfL1_Remotest<td><td>C+<td>Deduces statistical bucket-based rules, rather than position-based, but they are better than chance, because bucket and position correlate
<tr><td>quadMixed1<td><td>A<td>
<tr><td>quadNearby<td><td>A<td>
<tr><td>sha1OrdBuck_qcts0213<td><td>A<td>
<tr><td>sha1OrdBuck_tqsc0213<td><td>B+<td><td>Claimed slightly more permissive rules than the real ones, and dismissed contradicting facts as "anomalies"
<tr><td>sha1Ord_csqt<td><td>C<td>
<tr><td>sha1Ord_qcts<td><td>C<td>
<tr><td>shaOrdL1_csqt<td><td>C<td>
<tr><td>shaOrdL1_qcts<td><td>C,B,C<td><td>One candidate figured the order of shapes, but not the position restriction
<tr><td>sm_csqt<td><td>A<td>
<tr><td>sm_qcts<td><td>A<td>
</table>

<p>
  Training on a smaller number of episodes. For each rule set, the same RG seed (seed=1) is used in all runs; therefore, the set of 3 episodes provided e.g. in the prepared-a-3 run is the same as the one provided in the prepared-b-3 run, and a subset of the set provided in the prepared-a-5 and prepared-b-5 runs.

  <table border=1>
<tr><td><th colspan=2>Instr. with examples <th colspan=5>Instructions w/o examples </tr>
<tr><td>Directory <td>prepared-5-a <td>prepared-10-a <td>prepared-1-b <td>prepared-2-b <td>prepared-3-b <td>prepared-5-b <td>prepared-10-b </tr>
<tr><td>Number of prepared episodes<td>5 <td>10 <td>1 <td>2 <td>3 <td>5 <td>10 
<tr><th align=left>allOfColOrd_BRKY<td>15 <td>0 <td>2 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>allOfColOrd_KRBY<td>5 <td>13 <td>3 <td>1 <td>1 <td>0 <td>0 </tr>
<tr><th align=left>allOfShaOrd_csqt<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>allOfShaOrd_qcts<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>buckets_2130<td>15 <td>10 <td>15 <td>10 <td>10 <td>10 <td>15 </tr>
<tr><th align=left>ccw<td>5 <td>15 <td>15 <td>15 <td>15 <td>10 <td>15 </tr>
<tr><th align=left>cm_KRBY<td>15 <td>10 <td>10 <td>15 <td>10 <td>10 <td>9 </tr>
<tr><th align=left>cm_RBKY<td>15 <td>15 <td>10 <td>15 <td>10 <td>7 <td>15 </tr>
<tr><th align=left>cm_RBKY_cw_0123<td>3 <td>0 <td>0 <td>0 <td>1 <td>1 <td>0 </tr>
<tr><th align=left>col1OrdBuck_BRKY0213<td>1 <td>0 <td>1 <td>0 <td>1 <td>0 <td>0 </tr>
<tr><th align=left>col1OrdBuck_BRKY3120<td>1 <td>0 <td>0 <td>0 <td>2 <td>0 <td>0 </tr>
<tr><th align=left>col1Ord_BRKY<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>col1Ord_KRBY<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>colOrdL1_BRKY<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>colOrdL1_KBYR<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>cw<td>15 <td>15 <td>15 <td>15 <td>15 <td>15 <td>10 </tr>
<tr><th align=left>cw_0123<td>15 <td>15 <td>15 <td>10 <td>2 <td>12 <td>10 </tr>
<tr><th align=left>ordL1<td>0 <td>1 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>ordL1_Nearby<td>0 <td>0 <td>0 <td>0 <td>5 <td>0 <td>5 </tr>
<tr><th align=left>ordRevOfL1<td>2 <td>1 <td>0 <td>2 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>ordRevOfL1_Nearby<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>ordRevOfL1_Remotest<td>0 <td>0 <td>0 <td>5 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>quadMixed1<td>10 <td>0 <td>10 <td>15 <td>15 <td>10 <td>10 </tr>
<tr><th align=left>quadNearby<td>10 <td>15 <td>15 <td>15 <td>11 <td>9 <td>14 </tr>
<tr><th align=left>sha1OrdBuck_qcts0213<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>sha1OrdBuck_tqsc0213<td>0 <td>0 <td>0 <td>0 <td>0 <td>1 <td>0 </tr>
<tr><th align=left>sha1Ord_csqt<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>sha1Ord_qcts<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>shaOrdL1_csqt<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>shaOrdL1_qcts<td>0 <td>0 <td>0 <td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>sm_csqt<td>10 <td>8 <td>0 <td>15 <td>2 <td>10 <td>0 </tr>
<tr><th align=left>sm_qcts<td>10 <td>10 <td>5 <td>15 <td>15 <td>5 <td>4 </tr>
</table>

<p>To ward off Paul's suggestion that the bot perhaps may somehow "transfer learning between runs", we also had a series of experiments where a different RG seed was used for each run. It showed no quantitive difference from the above.

<table border=1>
<tr><td><th colspan=4>Instructions w/o examples </tr>
<tr><td>Directory <td>prepared-1-b <td>prepared-2-b <td>prepared-3-b <td>prepared-5-b </tr>
<tr><td>Number of prepared episodes<td>1 <td>2 <td>3 <td>5 
<tr><th align=left>allOfColOrd_BRKY<td>3 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>allOfColOrd_KRBY<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>allOfShaOrd_csqt<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>allOfShaOrd_qcts<td>0 <td>0 <td>3 <td>4 </tr>
<tr><th align=left>buckets_2130<td>10 <td>10 <td>14 <td>15 </tr>
<tr><th align=left>ccw<td>15 <td>15 <td>15 <td>15 </tr>
<tr><th align=left>cm_KRBY<td>10 <td>15 <td>14 <td>10 </tr>
<tr><th align=left>cm_RBKY<td>15 <td>9 <td>13 <td>15 </tr>
<tr><th align=left>cm_RBKY_cw_0123<td>0 <td>0 <td>4 <td>0 </tr>
<tr><th align=left>col1OrdBuck_BRKY0213<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>col1OrdBuck_BRKY3120<td>1 <td>0 <td>0 <td>2 </tr>
<tr><th align=left>col1Ord_BRKY<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>col1Ord_KRBY<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>colOrdL1_BRKY<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>colOrdL1_KBYR<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>cw<td>15 <td>15 <td>15 <td>15 </tr>
<tr><th align=left>cw_0123<td>15 <td>15 <td>15 <td>15 </tr>
<tr><th align=left>ordL1<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>ordL1_Nearby<td>4 <td>0 <td>5 <td>0 </tr>
<tr><th align=left>ordRevOfL1<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>ordRevOfL1_Nearby<td>0 <td>8 <td>0 <td>0 </tr>
<tr><th align=left>ordRevOfL1_Remotest<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>quadMixed1<td>5 <td>10 <td>11 <td>11 </tr>
<tr><th align=left>quadNearby<td>15 <td>15 <td>15 <td>15 </tr>
<tr><th align=left>sha1OrdBuck_qcts0213<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>sha1OrdBuck_tqsc0213<td>1 <td>1 <td>0 <td>0 </tr>
<tr><th align=left>sha1Ord_csqt<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>sha1Ord_qcts<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>shaOrdL1_csqt<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>shaOrdL1_qcts<td>0 <td>0 <td>0 <td>0 </tr>
<tr><th align=left>sm_csqt<td>0 <td>11 <td>15 <td>15 </tr>
<tr><th align=left>sm_qcts<td>10 <td>10 <td>12 <td>10 </tr>
</table>



</body>
</html>
